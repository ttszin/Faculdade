# -*- coding: utf-8 -*-
"""trabalho_aplicado_134498.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mzaTV7TKSq6eLInqAYatDUC17wcadj_9

**1) Utilize os métodos numéricos de resolução de equações para determinar o valor de Vbe que satisfaz a equação dada**

**a) BISSEÇÃO**
"""

#definir a função a ser encontrada a raiz
import numpy as np
import math

#Definindo as variáveis
iss = 10e-14
vt = 0.026
ib = 10e-2

#definir uma função para aplicar o método da bisseção
#função, intervalo inicial e final, tolerância desejada e o nº máximo de iterações

def bissecao(f,xl,xu,cp,maxiter):
  iter = 0 #inicializando o contador de iterações
  #Enquanto não chegar no número máximo de iterações
  while (iter<maxiter):
    xr = (xl+xu)/2 #calcula o ponto médio
    print(xr) #Imprime os valores em cada iteração
    f_xr = f(xr)

    if abs(f_xr) < cp: #verificando se a tolerancia foi satisfeita
      return xr #retorna xr como raiz aproximada

    if ((f(xl)*f_xr)<0): # se for menor que zero então
      xu = xr # xu recebe xr porque a raiz está no intervalo inferior
    else:
      xl=xr # xl recebe xr porque a raiz está no intervalo superior

    iter = iter+1 #Adiciona a iteração

  return print("Não atingiu a tolerância desejada dentro do numero maximo de iteraçoes")




#Declarando a função f
def f(vbe):
  return iss*(math.exp(vbe / vt) - 1) - ib

xl = 0.5 # estimativa do intervalo inferior
xu = 0.9 # estimativa do intervalo superior

cp = 0.01

maxiter = 30 #número máximo de iterações

raiz_aprox = bissecao(f,xl,xu,cp,maxiter)
raiz_aprox

"""**b) PONTO FIXO**"""

#O código abaixo tem como objetivo a implementação do método do ponto fixo


#Importação das bibliotecas utilizadas no código
import numpy as np
from math import e

#Declarando as variáveis
iss = 10e-14
vt = 0.026
ib = 10e-2

################################################################################
                          #definir a função g(x)
                          #escolher aproximação x0
                          #definir o critério de parada (tol)
                          #definir o máximo de iterações (maxiter)
                          #por fim aplicar o método do ponto fixo
################################################################################

#Definindo a função g(x)
def g(x):
  return (vt * math.log(ib / iss + 1))      #Função utilizada no exemplo

#definindo o método do ponto fixo
def fp(tol,maxiter,x0):
  i=0  #Define o contador como 0
  while (i<maxiter):  #Enquanto o contador for maior que o máximo de iterações
    x1 = g(x0) #Calcula a nova estimativa da raiz

    absolute_error = abs(x1-x0)   #Calcula o erro absoluto
    relative_error = absolute_error / abs(x1) * 100 #Utiliza a variável de erro absoluto para calcular o erro relativo
    print("Iteração", i+1, "- Aproximação:", x1, "- Erro relativo:", relative_error, "%")  #Mostra o valor do erro em cada iteração
    if (absolute_error < tol):  #Confere se o erro é menor que a tolerância
      return x1                 #e retorna x1 como a aproximação se for verdadeiro
    x0 = x1                     #Atualiza o valor de x0 para nova iteração
    i=i+1                       #Incrementa 1 no contador
  return print('Não encontrada uma raiz nos limites definidos, por favor aumente o número de iterações. ')  #Em caso de poucas iterações  máximas
                                                                                                            #retorna que não conseguiu encontrar nesse determinado número de iterações

maxiter = 100
tol = 1e-5
x0 = 0

print("Aproximação da raiz:", fp(tol,maxiter,x0))

"""**c) NEWTON RAPHSON**"""

#Importando as bibliotecas usadas

from math import e
import numpy as np


################################################################################
              # argumentos da função de newton (x0,f,df,tol,maxiter)
              # x0 = chute inicial
              # maxiter = máximo de iterações
              # df = derivada da função
################################################################################



def f(x):             #Define a função f(x)
    return iss*(math.exp(vbe / vt) - 1) - ib

def df(x):            #define a derivada de f
    return (iss / vt) * math.exp(vbe / vt)

#define o método de newton raphson
def newton_raphson(x0,tol,maxiter):
  i=0     # define o contador como 0
  while(i<maxiter):  #faz o loop mirando o número de iterações
    f_x0 = f(x0)     #Atribui f(x0) a f_x0
    df_x0 = df(x0)   #chama o método da derivada de f e atribui a df_x0

    x1 = x0 - f_x0 / df_x0  #Calcula a aproximação de x1

    erro = abs(x1-x0)/abs(x1) * 100   #Calcula o erro relativo

    print("Iteração ",i+1,"- Aproximação: ",x1 ," - Erro relativo: ",erro) #Printa a cada Iteração o seu número, aproximação e o erro relativo
    if(erro < tol):                                   #Se o erro relativo das iterações ou f(x1) for menor que a tolerância retorna x1
      return x1
    x0=x1                                                                  #Atribui um novo valor para x0 afim de continuar a iterações
    i = i+1                                                                #Incrementa no contador

  print("Erro, não convergiu dentro do número máximo de iterações determinado, por favor aumente o número de iterações.")  #Imprime um erro em caso de o número máximo de iterações ser insuficiente para calcular

vbe = 0.7                                                 # Definindo o valor de x0
maxiter = 100                                           # Definindo o valor máximo de iterações
tol = 1                                                 # Definindo a tolerância

raiz = newton_raphson(x0, tol, maxiter)                 # Chama o método
print("Raiz encontrada:", raiz)                         # Imprime a raiz final encontrada

"""**d) Método das Secantes**"""

#Importando a biblioteca math
from math import e

def f(x_value):                      #declarando a função f(x)
  return e**(-x_value) - x_value     # A função retorna a mesma definida nos slides de exemplo

################################################################################
                  # argumentos da função de newton (x0,f,df,tol,maxiter)
                  # x0 = chute inicial
                  # maxiter = máximo de iterações
                  # df = derivada da função
################################################################################



erro = 100   #Setando o erro inicial como 100
#define o método da secante
def secant(x0,x1,tol,maxiter):
  i=0     # define o contador como 0
  while(i<maxiter):  #faz o loop mirando o número de iterações
    f_x0 = f(x0)     #Atribui f(x0) a f_x0
    f_x1 = f(x1)     #Atribui f(x1) a f_x1

    x2 = x1 - (f_x1*(x0-x1)/(f_x0-f_x1))          #Calcula o valor de x2 utilizando a fórmula


    erro = abs((x2-x1)/x2) * 100   #Calcula o erro relativo

    print("Iteração ",i+1,"- Aproximação: ",x2 ," - Erro relativo: ",erro) #Printa a cada Iteração o seu número, aproximação e o erro relativo
    if(erro < tol):                                   #Se o erro relativo das iterações ou f(x1) for menor que a tolerância retorna x2
      return x2                                       #Retornando x2
    x0=x1          #Atribui um novo valor para x0 afim de continuar a iterações
    x1=x2          #Atribui um novo valor para x0 afim de continuar a iterações
    i = i+1        #Incrementa no contador

  print("Erro, não convergiu dentro do número máximo de iterações determinado, por favor aumente o número de iterações.")  #Imprime um erro em caso de o número máximo de iterações ser insuficiente para calcular

#Chamando o método:
x0 = 0.5      #definindo o valor inicial de x0
x1 = 0.9      #definindo o valor inicial de x1
tol = 1.0   #definindo a tolerância
maxiter = 100  #definindo o máximo de iterações


raiz = secant(x0,x1,tol,maxiter)  #chamando o método com seus argumentos
print("Raiz encontrada: ",raiz)   #Por fim, imprime a raiz final

"""#QUESTÃO2

**a) Pelo método da eliminação Gaussiana**
"""

#Importando as bibliotecas necessárias
import numpy as np
import math

#Definindo o método
def eliminacao_de_gauss(A,b):
    n, _ = np.shape(A)                                  #Pega o número de linhas da matriz A inicial
    matriz_aumentada = np.concatenate((A,b),1)          #Coloca a matriz b ao lado da matriz a
    k = 0                                               #Define k=0
    #Faz o loop para percorrer os pivôs
    for k in range(n-1):
        pivo = matriz_aumentada[k,k]        #Calcula o pivô da iteração
        #Percorre as linhas abaixo do pivô
        for i in range(k+1,n):
            matriz_aumentada[i,k]   #numero abaixo do pivo
            fator = matriz_aumentada[i,k]/pivo
            matriz_aumentada[i,k] = matriz_aumentada[i,k] - (fator * pivo) #atribui o novo valor para o número abaixo do pivô
            #Percorre as colunas depois do pivo
            for j in range(k+1,n+1):
                #Faz o cálculo do novo a(n)
                matriz_aumentada[i,j] = matriz_aumentada[i,j] - (fator * matriz_aumentada[k,j])
    print(f"Matriz aumentada:\n{matriz_aumentada}\n")
    return matriz_aumentada

#Definindo a substituição regressiva
def substituicao_regressiva(matriz_aumentada):
    n, m = matriz_aumentada.shape  # Obtém o número de linhas e colunas da matriz aumentada
    x = np.zeros(n)  # Inicializa o vetor solução com zeros
    print(f"Valores de X :\n")
    # Itera sobre as linhas da matriz aumentada de baixo para cima (n-1 até 0)
    for i in range(n-1, -1, -1):
        # Começa assumindo que o valor de x[i] é igual ao termo independente da linha i
        x[i] = matriz_aumentada[i, -1]

        # Para cada coluna à direita da diagonal, subtrai o produto do coeficiente da matriz e o valor já encontrado de x[j]
        for j in range(i+1, n):
            x[i] = x[i] - matriz_aumentada[i, j] * x[j]

        # Divide pelo coeficiente diagonal para isolar x[i]  (isola)
        x[i] = x[i] / matriz_aumentada[i, i]

        #Printando os valores de x[i]
        print(f"X{i+1} = {x[i]}")

    # Retorna o vetor solução
    print(f"\nVetor solução:")
    return x

#Apenas declarando e chamando as funções
A = np.array([
    [0.30,0.27,0.15],
    [0.25,0.40,0.20],
    [0.15,0.27,0.30]
])

b = np.array([
    [200],
    [250],
    [180]
])

#Chamando a função da eliminação de Gauss e atribuindo a normalização a matriz_aumentada
(matriz_aumentada) = eliminacao_de_gauss(A,b)
#Chamando a substituição regressiva desta matriz aumentada
solucao = substituicao_regressiva(matriz_aumentada)
print(solucao)   #Printando o vetor solução

"""**b) Pelo método de JACOBI**"""

import numpy as np

#Função que confere se a matriz é diagonalmente dominante
def is_diagonally_dominant(A):
    """
    Verifica se a matriz A é diagonalmente dominante.

    Parâmetro:
    A (ndarray): Matriz dos coeficientes.

    Retorna:
    bool: True se a matriz for diagonalmente dominante, False caso contrário.
    """
    n = A.shape[0]  #Pega o número de linhas da matriz A
    #Percorre as linhas na matriz
    for i in range(n):
         # Realiza a soma dos valores absolutos dos elementos fora da diagonal principal na linha i
        soma = sum(abs(A[i, j]) for j in range(n) if i != j)
        # Confere se o valor absoluto do elemento da diagonal principal é menor que a soma dos valores absolutos dos outros elementos
        if abs(A[i, i]) < soma:
            return False
    return True

#Declarando o método
def jacobi_richardson(A, b, x, n, eps, iter_max):
    """
    Implementação do método de Jacobi-Richardson.

    Parâmetros:
    A (ndarray): Matriz dos coeficientes.
    b (ndarray): Vetor constante.
    x (ndarray): Aproximação inicial dos valores de x.
    n (int): Tamanho da matriz (número de linhas ou colunas de A).
    eps (float): Tolerância para o critério de parada.
    iter_max (int): Número máximo de iterações permitidas.

    Retorna:
    x (ndarray): Aproximação para a solução do sistema.
    """
    #Confere se a matriz é diagonalmente dominante ou não, caso não retorna que o método pode não convergir
    if not is_diagonally_dominant(A):
        raise ValueError("A matriz não é diagonalmente dominante. O método de Jacobi-Richardson pode não convergir.")
    #Começa a iterar em 0
    iter = 0
    #Define a variável do x antigo
    x_antigo = np.copy(x)

    #Enquanto não atingir o número máximo de iterações ele continua
    while iter < iter_max:
        #Percorre as linhas
        for i in range(n):
            soma = 0
            #Percorre as colunas
            for j in range(n):
                #Ignora os elementos da diagonal principal na soma
                if i != j:
                    soma += A[i, j] * x_antigo[j]
            #Calcula o novo valor de x na posição i
            x[i] = (b[i] - soma) / A[i, i]

        # Verifica o critério de parada usando o erro relativo percentual
        erro_relativo = np.linalg.norm(x - x_antigo) / np.linalg.norm(x)
        #Confere se o erro relativo é menor que a tolerância
        if erro_relativo < eps:
            return x
        #Atribui um novo valor ao x antigo
        x_antigo = np.copy(x)
        iter += 1   #Adiciona 1 para ir para a próxima iteração (i)

    return x

#Declarando a matriz A e o vetor B
A = np.array([
    [0.30,0.27,0.15],
    [0.25,0.40,0.20],
    [0.15,0.27,0.30]
])

b = np.array([
    [200],
    [250],
    [180]
],dtype=float)
n = len(b)   #Define o tamanho do sistema
x_inicial = np.zeros_like(b) #Define a aproximação inicial com um vetor de zeros
eps = 0.01  # 1% de erro relativo percentual
iter_max = 1000  #Máximo de iterações

# Chamando a função
try:  #Tenta chamar a função
    x_solucao = jacobi_richardson(A, b, x_inicial, n, eps, iter_max)
    print("A solução é:", x_solucao)
#Se houver erro mostra o erro ocorrido
except ValueError as e:
    print(e)

"""**c) Gauss-Seidel**"""

import numpy as np

#Declarando a matriz A e o vetor B
A = np.array([
    [0.30,0.27,0.15],
    [0.25,0.40,0.20],
    [0.15,0.27,0.30]
])

b = np.array([
    [200],
    [250],
    [180]
],dtype=float)

# Inicializando o vetor x com zeros (chute inicial)
x = np.zeros_like(b)  # x = [0, 0] é o chute inicial

# Definindo a tolerância e o número máximo de iterações
tolerancia = 1e-10  # Critério de parada com base na precisão desejada
max_iter = 100  # Número máximo de iterações permitidas

n = len(b)  # Número de equações (ou o tamanho do vetor b)

# Loop principal para iterações do método de Gauss-Seidel
for k in range(max_iter):
    x_old = np.copy(x)  # Mantém a cópia dos valores antigos de x para o critério de parada

    # Loop interno para calcular cada elemento de x
    for i in range(n):
        soma = 0  # Inicializa a soma para o cálculo do termo x[i]

        # Calcula a soma dos produtos A[i, j] * x[j] para todos os j diferentes de i
        for j in range(n):
            if i != j:  # Não inclui o termo da diagonal principal A[i, i]
                soma += A[i, j] * x[j]

        # Atualiza o valor de x[i] usando os valores mais recentes de x
        x[i] = (b[i] - soma) / A[i, i]

    # Verifica o critério de parada (se a diferença entre x e x_old é menor que a tolerância)
    if np.linalg.norm(x - x_old, ord=np.inf) < tolerancia:
        break  # Se o critério de parada é atendido, sai do loop

# Imprime a solução encontrada após as iterações
print("Solução encontrada:", x)

"""**d) Explicar a diferença entre os métodos iterativos (Jacobi e Gauss-Seidel) e o método direto (Eliminação Gaussiana) em termos de aplicabilidade, convergência e complexididade computacional**

A diferença é que o método direto (eliminação gaussiana) é aplicável em sistemas pequenos e médios, devido ao fato de a resolver o sistema linear de forma exata em um número finito de iterações, exigindo uma grande quantidade de operações, o que torna não indicado para sistemas muito grandes, por outro lado, em sistemas menores geralmente é importante ter eficiência e precisão, então o método direto possui melhores resultados.

Enquanto que os métodos diretos (Jacobi e Gauss-Seidel) costumam ser mais utilizados em sistemas grandes e esparsos, porém não tem a convergência garantida, e também não possuem uma solução exata.São aplicáveis em sistemas onde a matriz é diagonalmente dominante, nesses casos eles garantem convergência.

O método de Gauss-Seidel geralmente converge mais rápido que o de Jacobi, porém isso irá depender do sistema analisado, e nem sempre será assim.

O método da eliminação Gaussiana tem complexidade de O(n^3), enquanto que o de Jacobi e Gauss-Seidel tem a complexidade de O(n^2) por iteração.

**e) Discutir a influência dos parâmetros do sistema (porcentagens de cada tipo de cabo) na convergência dos métodos iterativos. Como esses parâmetros podem afetar a estabilidade e a velocidade de convergência dos métodos:**

A convergência e a estabilidade dos métodos iterativos, como o método de Jacobi, são fortemente influenciadas pelos parâmetros do sistema, como os coeficientes da matriz associada ao sistema linear.

Um fator crucial para a convergência é a **diagonal dominante** da matriz, onde cada elemento da diagonal principal deve ser maior do que a soma dos valores absolutos dos outros elementos na mesma linha. Quando isso ocorre, o método de Jacobi tende a convergir de forma mais rápida e estável. Caso contrário, a convergência pode ser lenta ou até inexistente.

A **magnitude dos coeficientes** também afeta a convergência: coeficientes grandes permitem correções mais significativas a cada iteração, acelerando o processo, enquanto coeficientes pequenos podem levar a uma convergência mais lenta. Além disso, uma **distribuição uniforme** dos coeficientes favorece a estabilidade do método, enquanto um desbalanceamento significativo pode causar oscilações ou dificultar a convergência.

O **condicionamento do sistema** é outro fator determinante. Sistemas bem condicionados, onde as variáveis são menos dependentes entre si, tendem a convergir mais rapidamente e de forma estável. Por outro lado, sistemas mal condicionados podem sofrer com erros numéricos, dificultando ou até impedindo a convergência.

Por fim, a robustez do método a **perturbações** depende da configuração dos coeficientes. Sistemas bem condicionados e com parâmetros ajustados adequadamente são mais resistentes a pequenas perturbações, garantindo uma convergência eficiente e precisa. Em resumo, garantir que esses parâmetros estejam configurados corretamente é essencial para o sucesso dos métodos iterativos.

**f)Suponha que você precise resolver o sistema (1) para um sistema com o dobro da quantidade de forneccedores. Qual o método você escolheria e por quê?**

Acredito que eu usaria o método de Gass-Seidel, por se sair melhor em sistemas maiores, além de possuir uma melhor convergência, pois as atualizações feitas em cada iteração são utilizadas imediatamente nas próximas variáveis, sendo bastante eficiente em termos de memória. Da mesma forma, se a matriz ampliada melhora as características de diagonal dominante irá agilizar a resolução do sistema e o número de fornecedores não irá afetar ficando maior se isso ocorrer.
"""